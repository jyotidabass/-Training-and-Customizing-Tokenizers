{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPf8Xr3c1HA4WkHvQuLQKh4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyotidabass/-Training-and-Customizing-Tokenizers/blob/main/Training_and_Customizing_Tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Create dummy data\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"This is a sample sentence.\",\n",
        "        \"This is another sample sentence.\",\n",
        "        \"I love playing football.\",\n",
        "        \"I love playing basketball.\",\n",
        "        \"This is a new sentence with [NEW_TOKEN].\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert dummy data to pandas dataframe\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create a tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Print the original tokens\n",
        "print(\"Original Tokens:\")\n",
        "print(tokenizer.encode(df[\"text\"][0]))\n",
        "\n",
        "# Customize the tokenizer by adding new tokens\n",
        "tokenizer.add_tokens([\"[NEW_TOKEN]\"])\n",
        "\n",
        "# Print the customized tokens\n",
        "print(\"\\nCustomized Tokens:\")\n",
        "print(tokenizer.encode(df[\"text\"][4]))\n",
        "\n",
        "# Train the tokenizer on the dummy data using train_new_from_iterator\n",
        "# This method is used to train the tokenizer on new data.\n",
        "tokenizer.train_new_from_iterator(df[\"text\"], vocab_size=len(tokenizer)) #vocab_size can be adjusted\n",
        "#Print the trained tokens\n",
        "print(\"\\nTrained Tokens:\")\n",
        "print(tokenizer.encode(df[\"text\"][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZJDHOeLst7h",
        "outputId": "e2b7f412-3f5b-422d-f5b5-603c191b926d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens:\n",
            "[101, 2023, 2003, 1037, 7099, 6251, 1012, 102]\n",
            "\n",
            "Customized Tokens:\n",
            "[101, 2023, 2003, 1037, 2047, 6251, 2007, 30522, 1012, 102]\n",
            "\n",
            "Trained Tokens:\n",
            "[101, 2023, 2003, 1037, 7099, 6251, 1012, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here's a simple real-life application using Python code for Training  Tokenizers:"
      ],
      "metadata": {
        "id": "pa1bkjX3s1e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load the dummy dataset\n",
        "df = pd.DataFrame({\n",
        "    \"review\": [\n",
        "        \"I loved the movie!\",\n",
        "        \"It was a terrible film.\",\n",
        "        \"The acting was great.\",\n",
        "        \"The special effects were amazing.\",\n",
        "        \"I didn't like it.\"\n",
        "    ],\n",
        "    \"sentiment\": [1, 0, 1, 1, 0]\n",
        "})\n",
        "\n",
        "# Create a tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Preprocess the text data\n",
        "preprocessed_reviews = []\n",
        "for review in df[\"review\"]:\n",
        "    preprocessed_review = tokenizer.encode(review, add_special_tokens=True)\n",
        "    preprocessed_reviews.append(preprocessed_review)\n",
        "\n",
        "# Create a custom dataset class\n",
        "class MovieReviewDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, preprocessed_reviews, labels):\n",
        "        self.preprocessed_reviews = preprocessed_reviews\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\"input_ids\": self.preprocessed_reviews[idx], \"labels\": self.labels[idx]}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.preprocessed_reviews)\n",
        "\n",
        "# Create a dataset instance\n",
        "dataset = MovieReviewDataset(preprocessed_reviews, df[\"sentiment\"])\n",
        "\n",
        "# Create a DataLoader for batching\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=lambda batch: tokenizer.pad(batch, padding='longest', return_tensors='pt'))\n",
        "# Added collate_fn to pad sequences to the longest length in the batch\n",
        "\n",
        "# Train a model for sentiment analysis\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        # Move input_ids and labels to device within the loop\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Pass attention_mask to the model\n",
        "        outputs = model(input_ids, labels=labels, attention_mask=batch['attention_mask'].to(device)) # Pass attention_mask to model\n",
        "        loss = outputs.loss  # Access loss using outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")  # Divide by the number of batches\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "test_review = \"I loved the movie!\"\n",
        "test_review = tokenizer.encode(test_review, add_special_tokens=True)\n",
        "test_review = torch.tensor(test_review).unsqueeze(0).to(device)  # Add batch dimension\n",
        "output = model(test_review)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDXg4EKBrhLJ",
        "outputId": "7339e63b-89d4-418d-f91b-d6691af0647a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7540531555811564\n",
            "Epoch 2, Loss: 0.6956837773323059\n",
            "Epoch 3, Loss: 0.5696997046470642\n",
            "Epoch 4, Loss: 0.46580002705256146\n",
            "Epoch 5, Loss: 0.5019958118597666\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[0.0422, 0.6374]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here's a Python code with simplest explanation for Customizing Tokenizers:\n",
        "Customizing Tokenizers\n",
        "Goal: Customize a tokenizer to recognize specific words or phrases."
      ],
      "metadata": {
        "id": "pknKKnNhtaL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the dummy dataset\n",
        "df = pd.DataFrame({\n",
        "    \"text\": [\n",
        "        \"I loved the movie!\",\n",
        "        \"It was a terrible film.\",\n",
        "        \"The acting was great.\",\n",
        "        \"The special effects were amazing.\",\n",
        "        \"I didn't like it.\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Create a tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Customize the tokenizer\n",
        "tokenizer.add_tokens([\"[MOVIE]\", \"[ACTING]\"])\n",
        "\n",
        "# Preprocess the text data\n",
        "preprocessed_text = []\n",
        "for text in df[\"text\"]:\n",
        "    preprocessed_text.append(tokenizer.encode(text, add_special_tokens=True))\n",
        "\n",
        "# Print the preprocessed text\n",
        "print(preprocessed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYI901GHtQUF",
        "outputId": "a42a25ad-ec26-4e8e-a1d9-a910c77ba450"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[101, 1045, 3866, 1996, 3185, 999, 102], [101, 2009, 2001, 1037, 6659, 2143, 1012, 102], [101, 1996, 3772, 2001, 2307, 1012, 102], [101, 1996, 2569, 3896, 2020, 6429, 1012, 102], [101, 1045, 2134, 1005, 1056, 2066, 2009, 1012, 102]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customizing Tokenizers with Special Tokens\n",
        "\n",
        "Goal: Customize a tokenizer to recognize special tokens."
      ],
      "metadata": {
        "id": "VrjT02SqtnVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the dummy dataset\n",
        "df = pd.DataFrame({\n",
        "    \"text\": [\n",
        "        \"I loved the movie!\",\n",
        "        \"It was a terrible film.\",\n",
        "        \"The acting was great.\",\n",
        "        \"The special effects were amazing.\",\n",
        "        \"I didn't like it.\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Create a tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "# Customize the tokenizer with special tokens\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': [\"[CLS]\", \"[SEP]\"]})\n",
        "\n",
        "# Preprocess the text data\n",
        "preprocessed_text = []\n",
        "for text in df[\"text\"]:\n",
        "    preprocessed_text.append(tokenizer.encode(text, add_special_tokens=True))\n",
        "\n",
        "# Print the preprocessed text\n",
        "print(preprocessed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruuH6rQctoyc",
        "outputId": "83d353e6-20ec-4efc-eae8-acc75307290f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[101, 1045, 3866, 1996, 3185, 999, 102], [101, 2009, 2001, 1037, 6659, 2143, 1012, 102], [101, 1996, 3772, 2001, 2307, 1012, 102], [101, 1996, 2569, 3896, 2020, 6429, 1012, 102], [101, 1045, 2134, 1005, 1056, 2066, 2009, 1012, 102]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}